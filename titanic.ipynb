{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-18T17:10:22.713769Z","iopub.execute_input":"2022-10-18T17:10:22.714786Z","iopub.status.idle":"2022-10-18T17:10:22.719421Z","shell.execute_reply.started":"2022-10-18T17:10:22.714711Z","shell.execute_reply":"2022-10-18T17:10:22.718660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = pd.read_csv(\"../input/titanic/test.csv\")\ndata_train = pd.read_csv(\"../input/titanic/train.csv\")\ndata_train.head()\n#data_train.shape #(891, 12)\n#data_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:10:24.996292Z","iopub.execute_input":"2022-10-18T17:10:24.996736Z","iopub.status.idle":"2022-10-18T17:10:25.025565Z","shell.execute_reply.started":"2022-10-18T17:10:24.996697Z","shell.execute_reply":"2022-10-18T17:10:25.024432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that there are a lot of features, so we will make a list of them and we will use them next:","metadata":{}},{"cell_type":"code","source":"features = [col for col in data_train.columns]\nprint(features)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:10:27.731306Z","iopub.execute_input":"2022-10-18T17:10:27.731772Z","iopub.status.idle":"2022-10-18T17:10:27.738254Z","shell.execute_reply.started":"2022-10-18T17:10:27.731734Z","shell.execute_reply":"2022-10-18T17:10:27.736850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we will decide on the types of each feature and that means everything except int or double (numeric)","metadata":{}},{"cell_type":"code","source":"def findObj(features):\n    features_obj = []\n    for feat in features:\n        if data_train[feat].dtype == \"object\":\n            features_obj.append(feat)\n    return features_obj\nfindObj(features)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T18:37:31.679601Z","iopub.execute_input":"2022-10-18T18:37:31.680036Z","iopub.status.idle":"2022-10-18T18:37:31.690981Z","shell.execute_reply.started":"2022-10-18T18:37:31.679998Z","shell.execute_reply":"2022-10-18T18:37:31.689546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen right here, the findObj list may have Nan value because their type is object. So we will try to see where are the null values:","metadata":{}},{"cell_type":"code","source":"def where_null(data_train, features):\n    features_where_null = []\n    for feat in features:\n        if data_train[feat].isnull().sum() >= 1:\n            features_where_null.append(feat)\n    return features_where_null\nproblem_feat = where_null(data_train, features)\nproblem_feat","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:10:34.047630Z","iopub.execute_input":"2022-10-18T17:10:34.048106Z","iopub.status.idle":"2022-10-18T17:10:34.060181Z","shell.execute_reply.started":"2022-10-18T17:10:34.048069Z","shell.execute_reply":"2022-10-18T17:10:34.058861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we shall see how many null values are there:","metadata":{}},{"cell_type":"code","source":"for feat in problem_feat:\n    print(feat + \" \" + str(data_train[feat].isnull().sum()))","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:10:36.629621Z","iopub.execute_input":"2022-10-18T17:10:36.630355Z","iopub.status.idle":"2022-10-18T17:10:36.638612Z","shell.execute_reply.started":"2022-10-18T17:10:36.630304Z","shell.execute_reply":"2022-10-18T17:10:36.637705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that there are a lot of null values here. Now we compare these values to the number of actual value the column has:","metadata":{}},{"cell_type":"code","source":"for feat in problem_feat:\n    print(feat + \" \" + str(891 - data_train[feat].isnull().sum()))","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:10:38.523189Z","iopub.execute_input":"2022-10-18T17:10:38.523632Z","iopub.status.idle":"2022-10-18T17:10:38.531249Z","shell.execute_reply.started":"2022-10-18T17:10:38.523594Z","shell.execute_reply":"2022-10-18T17:10:38.530086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that 'Embarked' and 'Age' has some useful information. Comparing them to 'Cabin', these columns are more useful. But we should see how useful are they regarding to predicting the outcome (we can use mutual information or analysing the data based on percentile). For example:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5))\nplt.title(\"Based on gender\")\nsns.barplot(x = data_train['Sex'], y = data_train.index)\nplt.xlabel(\"Secs\")\nplt.ylabel(\"Number\")","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:10:40.837174Z","iopub.execute_input":"2022-10-18T17:10:40.837900Z","iopub.status.idle":"2022-10-18T17:10:41.089682Z","shell.execute_reply.started":"2022-10-18T17:10:40.837855Z","shell.execute_reply":"2022-10-18T17:10:41.088550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that there are slightly more men than women so it is balanced.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5), num = \"Based on Class\")\nsns.barplot(x = data_train['Pclass'], y = data_train.index)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:10:44.414354Z","iopub.execute_input":"2022-10-18T17:10:44.415420Z","iopub.status.idle":"2022-10-18T17:10:44.694289Z","shell.execute_reply.started":"2022-10-18T17:10:44.415376Z","shell.execute_reply":"2022-10-18T17:10:44.693172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classes are balanced; So now we will use mutual information to decide which data is more important regarding our model. When working with this data, it is important to separate the output('Survived') column to work with our data and to factorize the object type data because mutual information behaves differently when working with discrete features or continuos features","metadata":{}},{"cell_type":"code","source":"X = data_train.copy()\ny = X.pop(\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:52:40.434712Z","iopub.execute_input":"2022-10-18T17:52:40.435129Z","iopub.status.idle":"2022-10-18T17:52:40.441910Z","shell.execute_reply.started":"2022-10-18T17:52:40.435088Z","shell.execute_reply":"2022-10-18T17:52:40.440605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that \"Age\" haas null values in it. So we put the mean of the values in place of nan:","metadata":{}},{"cell_type":"code","source":"X['Age'].fillna(X['Age'].mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:52:42.747607Z","iopub.execute_input":"2022-10-18T17:52:42.748135Z","iopub.status.idle":"2022-10-18T17:52:42.755358Z","shell.execute_reply.started":"2022-10-18T17:52:42.748099Z","shell.execute_reply":"2022-10-18T17:52:42.754375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transformFare(X):\n    result = map(int, X['Fare'])\n    result = pd.Series(result)\n    #print(result)\n    X.drop(\"Fare\", axis = 'columns', inplace = True)\n    #X.head()\n    X.insert(8, \"Fare\", result)\n    \ndef transformAge(X):\n    result = map(int, X['Age'])\n    result = pd.Series(result)\n    X.drop(\"Age\", axis = \"columns\", inplace = True)\n    X.insert(4, \"Age\", result)\n    \ntransformFare(X)\ntransformAge(X)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:52:44.427009Z","iopub.execute_input":"2022-10-18T17:52:44.427741Z","iopub.status.idle":"2022-10-18T17:52:44.455104Z","shell.execute_reply.started":"2022-10-18T17:52:44.427698Z","shell.execute_reply":"2022-10-18T17:52:44.453965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So after preprocessing the data, we are ready to work with our dataset. We begin by implementing a mutual information function:","metadata":{}},{"cell_type":"code","source":"for cols in X.select_dtypes(\"object\"):\n    X[cols], _ = X[cols].factorize()\ndiscrete_features = X.dtypes == int\ndiscrete_features","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:55:24.694969Z","iopub.execute_input":"2022-10-18T17:55:24.695421Z","iopub.status.idle":"2022-10-18T17:55:24.706367Z","shell.execute_reply.started":"2022-10-18T17:55:24.695385Z","shell.execute_reply":"2022-10-18T17:55:24.704934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:57:24.287037Z","iopub.execute_input":"2022-10-18T17:57:24.287439Z","iopub.status.idle":"2022-10-18T17:57:24.365585Z","shell.execute_reply.started":"2022-10-18T17:57:24.287407Z","shell.execute_reply":"2022-10-18T17:57:24.364452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that sex and Pclass influence the most the probability of survival. So we decide that we can use a linear model, Logistic Regression. But firstly, let's split the data into train and test:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T19:27:10.867021Z","iopub.execute_input":"2022-10-18T19:27:10.868166Z","iopub.status.idle":"2022-10-18T19:27:10.875120Z","shell.execute_reply.started":"2022-10-18T19:27:10.868123Z","shell.execute_reply":"2022-10-18T19:27:10.874248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\ndef logis(X_train, y_train, X_test, y_test):\n    for c in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]:\n        logmodel = LogisticRegression(C = c, max_iter = 100000).fit(X_train, y_train)\n        print(\"The score for training {} is :{:.3f}\".format(c, logmodel.score(X_train, y_train)))\n        print(\"The score for testing {} is :{:.3f}\\n\".format(c, logmodel.score(X_test, y_test)))\n    \nlogis(X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T19:27:12.529664Z","iopub.execute_input":"2022-10-18T19:27:12.531003Z","iopub.status.idle":"2022-10-18T19:27:14.354070Z","shell.execute_reply.started":"2022-10-18T19:27:12.530960Z","shell.execute_reply":"2022-10-18T19:27:14.352658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that the best coefficients are the ones above 0.1 . It is not a model that performs poorly, but it can surely be improved. We could have used another methods for testing our model, like Leave One Out or shuffling the data, but it does not matter, since we have another data frame for testing the algorithm. But we shall see after testing it on the final dataset","metadata":{}},{"cell_type":"markdown","source":"Another model we can use is a SVM. It can be built in different methods by adjusting the parameters","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsvm = SGDClassifier(loss = 'modified_huber')\n \nsvm.fit(X_train, y_train)\nsvm.score(X_test, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-18T19:33:26.343706Z","iopub.execute_input":"2022-10-18T19:33:26.344122Z","iopub.status.idle":"2022-10-18T19:33:26.360394Z","shell.execute_reply.started":"2022-10-18T19:33:26.344090Z","shell.execute_reply":"2022-10-18T19:33:26.359289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}